---
title: "php2650_proj"
author: "Yifan Zhao, Zexuan Yu"
date: "12/05/2022"
output:  md_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/yvonne/Downloads/PHP2650SL/php2650_project")
# setwd("/Users/yvonne/Downloads/PHP2650SL/php2650_project")
library(tidyverse)
library(gridExtra)
library(knitr)
library(kableExtra)
library(survival)
library(data.table)
library(ranger)
library(caret)
#  pdf_document
```


**Reference:**
https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0383-8




# Survival Analysis

## Survival Data

* Goal: 
    * Assess the effect of risk factors on survival time - time until an event occurs, for example, death, diagnosis recovery, etc.
    * Estimate and interpret survival
    * Compare survival time / curve between groups

* Outcome: 
  * The outcome of interest is a combination of continuous variable represents time to event and a binary variable represents censoring status: Y = [T, C]  
  * T: Observed survival time
  * C: Censoring (or event) status
  

* Data snapshot:

```{r ,echo=FALSE}
include_graphics("figures/f1_data.png")
```



## Censoring

Consider $Y_i, U_i$ where $T_i$ is the time to event and $U_i$ is time to censoring, i.e. end of follow up period. We can only observe the minimum of the two time, whichever comes the first, $T_i = min(Y_i,U_i)$. Hence the actual exact survival time (T) is usually unknown, mainly due to that the follow-up period is limited. Some instances are like when there is no event happened up to the end of the follow-up period, researchers lose contact with the patient, patient withdrawal participation, or the exact date of the event is unknown. So we would use censoring to account for the missing data issue. There are three types of censoring:

* Right censoring: $T \geq T_E$ Event happened after study period
* Left censoring: $T \leq T_0$ Event happened before study period
* Interval censoring: $T_A \leq T \leq T_B$ Only know event time interval but don't know the exact time

T: actual survival time   
$T_0$: Start of study period   
$T_E$: End of study period  

```{r ,echo=FALSE}
include_graphics("figures/f2_censor.jpg")
```
*Image credit to PHP2514 course material, Dr. Chrysanthopoulou*

For the Random Survival Forest method, we mainly focus on the right-censoring data.

## Hazard Function

Survival analysis involves three characteristic functions: survival function, hazard function, and cumulative hazard function, of which we can derive all three given any one of the functions. The random survival forest algorithm uses the cumulative hazard function. Hazard function and cumulative hazard function are written as:

Hazard function (hazard rate): instantaneous potential per unit time for the event to occur at time t, given survival up to time t:  

<img src="https://render.githubusercontent.com/render/math?math=h(t) = lim \Delta t \rightarrow 0 : \frac {P(t\leq T<t+\Delta t|T\geq t)} {\Delta(t)}">

<!-- $$h(t) = lim \Delta t \rightarrow 0 : \frac {P(t\leq T<t+\Delta t|T\geq t)} {\Delta(t)}$$ -->


```{r ,echo=FALSE}
include_graphics("figures/f3_weibull.jpg")
```
*Image credit to PHP2514 course material, Dr. Chrysanthopoulou*


Cumulative Hazard Function: integrate hazard function h(t) from time 0 to time t 

$$H(t)=\int_{0}^{t} h(u) \,du$$


# Regression Models for Survival Data

## Cox Porportional Hazards Model (semi-parametric)


* Y: [T, C]
  * T: Observed survival time
  * C: Censoring (or event) status

* Hazard rate:

Assume $h_0(t)$ as fixed and $X_j$ are **time-independent**: 
<img src="https://render.githubusercontent.com/render/math?math=h(t)=h_0(t)e^{\sum_{j=1}^p\beta_jX_j}">

$$h(t)=h_0(t)e^{\sum_{j=1}^p\beta_jX_j}$$
    



## PH assumption

Cox proportional hazards model is usually used for right censored  time-to-event data. Mode is convenient for flexibility and simplicity, but it's restricted to *proportional hazards (PH) assumptions*, which is to assume hazard ratio as independent of time t (constant over time). In other words, the hazard rate at any time point t for a set a covariate X* is **proportional** to the hazard rate at the same time point t for other set of covariate X.

Hazard Ratio $\theta$ is defined as:

<img src="https://render.githubusercontent.com/render/math?math=HR=\frac{h(t,X^*)}{h(t,X)}=e^{\sum_{j=1}^p\beta_j(X_j^*-X_j)}=\theta">

$$HR=\frac{h(t,X^*)}{h(t,X)}=e^{\sum_{j=1}^p\beta_j(X_j^*-X_j)}=\theta$$

Graphically, this is represented as roughly parallel survival curves between different categories of a covariate. For example, the graph below shows parallel pattern between high and low WBC, but non-parallel between medium and the others.


```{r ,echo=FALSE}
include_graphics("figures/f4.png")
```


## Survival Trees as alternative to Cox PH

When PH assumption is violated, which is most of the cases for real life data, we turn to survival trees and Random survival forest (RSF) models as alternative methods to the Cox PH model. (BMC Medical paper)

In contrast of the semi-parametric Cox PH model, survival trees is a fully *non-parametric* method, which is much more flexible and can easily handle high dimensional covariate data. And it can automatically detect certain types of **interactions** without the need to specify them beforehand. However, the drawback of survival trees is that it tends to be biased towards inclusion of variables with many split points. 

### Conditional Inference Forest (CIF)*:


"The random survival forests algorithm, has been criticised for having a bias towards selecting variables with many split points and the conditional inference forest algorithm has been identified as a method to reduce this selection bias. 
". As a further alternative to survival trees, CIF is able to correct bias in RSF "by separating the algorithm for selecting the best covariate to split on from that of the best split point search" (BMC paper)







-------------------------------

# Survival Random Forest

Idea: partitioning the covariate space recursively to form groups of subjects who are similar according to the time-to-event outcome.
Minimizing a given impurity measure.
Goal: To identify prognostic factors that are predictive of the time-to-event outcome. 


## 1.	Introduction

Random survival forest (RSF) is a random forest method used to analyze right deletion survival data. It introduces new survival splitting rules for growing survival trees and new missing data algorithms for estimating missing data.
RSF introduced the event retention principle for living forests and used it to define overall mortality, which is a simple interpretable mortality measure that can be used as a predictive result. R package “randomSurvivalForest” provides an interface to use.

## 2.	RSF framework

(1) Extract $B$ bootstrap samples from the original data, and each bootstrap sample excludes 37% of the data on average, which is called out-of-bag data (OOB data).

(2) Construct a binary survival tree for each bootstrap sample. At each node of the tree, p candidate variables are randomly selected, and the node is split by using the candidate variables that maximize the survival difference between the child nodes.

(3) Grow the tree to full size with at least $d_0>0$ number of events (deaths).

(4)	Calculate cumulative risk function for each tree and obtain the mean value of the integrated cumulative risk function.

(5) Calculate the integrated cumulative risk function prediction error with OOB data.

## 3.	Ensemble cumulative hazard

Regenerating the survival tree and building the integrated CHF (Cumulative Hazard Function) are the central elements of the RSF algorithm. 

### Binary Survival tree

Like CART, a survival tree is a binary tree generated by recursively splitting tree nodes. A tree grows from the root node, which is the top of the tree that contains all the data. Using predetermined survival criteria, the root node is divided into two children: left and right. In turn, each child node is split, producing left and right child nodes with each split. The process is repeated recursively for each subsequent node.

A good node segmentation can maximize the survival difference between the offspring. The optimal split for a node can be found by searching all possible x variables and split values c and selecting the x and c that maximize the survival difference. By maximizing survival differences, trees separate out different situations. Eventually, as the number of nodes increased, the alien cases were separated, and each node in the tree became homogenous, made up of samples with similar survival rates.

### Node Segmentation Rules

At each node, a predictive variable $x$ and a partition value $c$ are randomly selected, $c$ being some value of $x$. If $x_i<c$, then the sample $i$ is divided into the right child node; If $x_i>c$, the sample $i$ is assigned to the left child node.

Calculate log rank test:

$L(x,c)=\frac{\sum\limits_{i=1}^{N}(d_{i,1}-Y_{i,1}\frac{d_{i,1}}{Y_{i,1}})}{\sqrt{\sum\limits_{i=1}^{N}\frac{Y_{i,1}}{Y_{i}}(1-\frac{Y_{i,1}}{Y_{i}})\frac{Y_{i}-d_{i}}{Y_{i}-1}d_{i}}}$

Where, $j={1,2}$ represents left and right child nodes,

$d_{i,j}$ is the number of events occurring in the sub-node $j$ at the time $t_i$,

$Y_{i,j}$ is the number of all patients in the sub-node $j$ at the moment $t_i$,

$d_i$ is the number of default events occurring at the time $t_i$, $d_i=∑_jd_{i,j}$ ,

$Y_i$ is the number of all borrowers at the moment $t_i$, $Y_i=∑_jY_{i,j}$ .

Iterate over all possible variables $x$ and partition values $c$, find variables $x$ and partition values $c$ that satisfies
                                   
$|L(x^*,c^*)|≥|L(x,c)|$
                                   
for all $x^*$ and $c^*$.

### Leaf node prediction

Eventually, the survival tree reaches a saturation point at which no new child nodes can be formed, because each node must contain at least $d_0>0$ of unique death criteria. The last node in a saturated tree is called a leaf node, denoted by $H$.

Let $(T_{(1,h)},δ_{(1,h)})$,…,$(T_{(n(h),h)},δ_{(n(h),h)})$ represent the sample survival time and 0-1 deletion information at the leaf node $h∈H$. $δ_{i,h}=0$ represents the right deletion of sample $T_{i,h}$ at moment $i$; $δ_{i,h}=1$ indicates that the sample $T_{i,h}$ occurs at time $i$.

$t_{1,h}<t_{2,h}<⋯<t_{N(h),h}$ represents $N(h)$ different moments when the event occurs; $d_{l,h}$ represents the number of deaths at the time $t_{l,h}$; $Y_{l,h}$ represents the number of people who are alive at the moment $t_{l,h}$; The cumulative risk function of the leaf node $h$ is estimated as Nelson-Aalen estimate:

$\hat{H}_h(t)=\sum\limits_{t_(l,h)≤t}\frac{d_{l,h}}{Y_{l,h}} =\sum\limits_{t_(l,h)≤t}\frac{who\;died\;at\;time\;l}{who\;known\;still\;survived\;to\;time\;l}$

The cumulative risk function is the same for all samples in the leaf node $h$. Each sample $i$ has the d-dimension covariant $X_i$, and $H(t│X_i)$ represents the cumulative risk function of sample $i$, and we have:

$H(t│X_i)=\hat{H}_h(t),if X_i∈h$

We have an example here to make our audience clear:

We have a table below:

In the table above, $i$ represents patient $i$, $t_i$ represents survival time of patient $i$, where 27+ indicates that the patient exited at moment 27, which belongs to deletion, and it is not known how long the patient survives.

$t_i$ from small to large order is 5, 27+, 30, 32, 35, 35, 40, then:

Those who survive to time 5 was $i=1,2,3,4,5,6$, people who died at time 5 was $i=6$;

Those who survive to 30 hours was $i=1,2,4,5$, people who died at time 30 was $i=4$;

Those who survive to time 32 was $i=1,2,5$,  people who died at time 32 was $i=2$.

So, we have the Nelson-Aalen estimate at time 33:

$H(33)=\sum\limits_{i=0}^{33}\frac{d_i}{n_i}=\frac{d_5}{n_5}+\frac{d_30}{n_30}+\frac{d_33}{n_33} =\frac{1}{6}+\frac{1}{4}+\frac{1}{3}=0.75$

### Bootstrap and OOB integrate cumulative risk functions

The cumulative risk function:

$H(t│X_i)=\hat{H}_h(t),if X_i∈h$

We got above is derived only from a single tree, and we need to calculate the integrated cumulative risk function based on the mean of total $B$ survival trees.

Assume that the cumulative risk function of the growth tree of the bootstrap sample $b$ is $H_b^* (t|X)$.

And we also assume that $I_{i,b}=1$ indicates that $i$ is a case in $b$th bootstrap sample, otherwise $I_{i,b}=0$.

The integrated cumulative risk function of the ith sample from OOB (out-of-bag) is:

$H{^{**}_{e}}(t│X_i)=\frac{\sum\limits_{b=1}^BI_{i,b}H_b^* (t|X_i)}{\sum\limits_{b=1}^BI_{i,b}}$

And the integrated cumulative risk function of the ith sample from IB (in-bag) is:

$H_e^*(t│X_i)=\frac{1}{B}\sum\limits_{b=1}^BH_b^*(t|X_i)$

## 4.	Ensemble mortality

In the Random Survival Forest, mortality is defined as the sum of expected value of the cumulative risk function over time $T_j$, subject to a specific $X_i$. Under the null hypothesis of similar survival behavior, it measures the expected number of deaths. Specifically, the mortality rate of $i$ is:

$M_i=E_i(\sum\limits_{j=1}^nH(T_j |X_i))$

where $E_i$ represents the expectation given the null hypothesis that all $j$ are similar to $i$.

Mortality can be estimated naturally in the survival tree model. The structure of survival tree enforces a null assumption that there are similar survival rates in its leaf nodes; Individuals within leaf nodes share a common estimated risk function. Thus, the nature of the survival tree and its integration indicates an estimate of mortality, which we call the ensemble mortality. The integrated mortality rate of the sample $i$ from IB (in-bag) is defined as:

$\hat{M}{^{*}_{ei}}=\sum\limits_{j=1}^n H{^{*}_{e}(T_j |X_i)}$

Similarly, the integrated mortality rate from the OOB sample $i$ is defined as:

$\hat{M}{^{**}_{ei}}=\sum\limits_{j=1}^n H{^{**}_{e}(T_j |X_i)}$


## 5.	Harrell’s C-index

The intuition behind Harrell’s C-index is as follows. For patient $i$, our risk model assigns a risk score $η_i$. If our risk model is any good, patients who had shorter times-to-disease should have higher risk scores. Boiling this intuition down to two patients: the patient with the higher risk score should have a shorter time-to-disease.

We can compute the C-index in the following way: For every pair of patients $i$ and $j$ (with $i≠j$), look at their risk scores and times-to-event.

(1)	If both $T_i$ and $T_j$ are not censored, then we can observe when both patients got the disease. We say that the pair $(i,j)$ is a concordant pair if and $T_i<T_j$, and it is a discordant pair if $η_i>η_j$ and $T_i>T_j$.

(2)	If both $T_i$ and $T_j$  are censored, then we don’t know who got the disease first (if at all), so we don’t consider this pair in the computation.

(3)	If one of $T_i$ and $T_j$  is censored, we only observe one disease. Let’s say we observe patient $i$ getting disease at time $T_i$, and that $T_j$ is censored. (The same logic holds for the reverse situation.)

(4)	If $T_i>T_j$, then we don’t know for sure who got the disease first, so we don’t consider this pair in the computation.

(5)	If $T_i<T_j$, then we know for sure that patient $i$ got the disease first. Hence, $(i,j)$ is a concordant pair if $η_i>η_j$, and is a discordant pair if $η_i<η_j$.

Thus we have C-index here:

$C=\frac{concordant\; pair}{concordant\; pair+ discordant\; pair}$

Values of$C$ near 0.5 indicate that the risk score predictions are no better than a coin flip in determining which patient will live longer. Values near 1 indicate that the risk scores are good at determining which of two patients will have the disease first. Values near 0 means that the risk scores are worse than a coin flip: you might be better off concluding the opposite of what the risk scores tell you.

### Harrell’s C-index for continuous data

Of course, one can compute the C-index if none of the data is censored. In that case, all pairs such that $T_i≠T_j$ will be included in the computation.

### Harrell’s C-index for binary data

The concept of the C-index can be easily ported over to binary data. In this setting, a high-risk score prediction means more likely to be 1 than a 0. We only consider pairs where subject $i$’s response is a 1 and subject $j$’s response is a one. The pair is concordant if $η_i>η_j$, and discordant if $η_i<η_j$.

We also have an example below




We have Concordant pairs：(A,C) (A,E) (C,D) so we can calculate the C-Index:

$C=\frac{3}{6}=0.5$

This means that the prediction is not much better than a random guess.




# Application in R


# Veteran Data

In this demo we're using ``veteran`` data from ``survival`` package, which records data of randomized trial of two treatment regimens for lung cancer. For model fitting, we need ``survival`` library for cox ph model and ``ranger`` library for random survival forest. 

```{r,echo=FALSE}
read.csv('figures/veteran.csv')  %>% kable(caption = 'Veteran Data')
library(data.table)
```

First, we split data into training set and test set:

```{r,message=FALSE,warning=FALSE}
library(survival)
library(ranger)
data(veteran)
veteran <- data.table(veteran)
vet = veteran
# Next, we split the data in a training and test set.
set.seed(123456)
ind <- sample(1:nrow(veteran),round(nrow(veteran) * 0.7,0))
veteran_train <- veteran[ind,]
veteran_test <- veteran[!ind,]
vet.tr = vet[ind,]
vet.te = vet[-ind,]
```


Use Kaplan Meier Curve as a visual summary of the survival probability between treatment groups:

```{r}
# plot survival curve:
kmvet = survfit(Surv(time, status)~trt, data=vet)
# km curve
plot(kmvet, col=c('blue','red'), xlab='Time', ylab='Survival Probability', main='Kaplan Meier Curves')
legend("topright", lwd = 1, col = c('blue','red'), cex=0.7, y.intersp = 0.5, legend = c('trt=1', 'trt=2'))
abline(h=0.5,lty=3)
```


## Cox PH Model

We first fit a COX PH Model. Using backward selection, we found the 'best' variable to fit the data. However, the log-log survival curves show non-parallel curves between variable groups. So the PH assumption might be violated. 

```{r, results=FALSE}
coxm0 = coxph(Surv(time, status)~(celltype+trt+karno+diagtime+age+prior)^2, data=vet.tr, ties='breslow')
# coxm1 = step(coxm0, direction = "backward")
# model selected from backward selection
coxm1 = coxph(formula = Surv(time, status) ~ celltype + trt + karno + diagtime + prior + 
    celltype:trt + trt:karno + trt:diagtime + trt:prior + karno:prior, data = vet.tr, ties = "breslow")
# summary(coxm1)
# PH assumption: non-parallel, violated
par(mfrow=c(2,2))
sapply(list(vet.tr$trt, vet.tr$celltype, vet.tr$karno, vet.tr$prior), 
       function(var) plot(survfit(Surv(time, status) ~ var, data=vet.tr),
                          col=1:10,
                          fun="cloglog",
                          ylab='log(-log(S(t)))', 
                          xlab='log(time)',
                          xlim=c(20,250),
                          ylim=c(-2,0.5),
                          main='Log-Log Survival Curves')
)
```

The goodness of fit test based on Schoenfeld Residuals is another way to test the PH assumption, which is the null hypothesis. However, we can see a lot of p values of less than 0.05, this validated our concern that the proportional hazard assumption is violated.

```{r}
cox.zph(coxm1) 
```


## Random Survival Models

### Use ranger package to train the random survival models

Recall that in random forest, we need to find number of variables **mtry ** to randomly select from at each node.  First, we find the optimal mtry parameter that gives the smallest OOB error. Over 1000 iterations, m = 3 has the highest frequency of giving the smallest OOB error. 

```{r}
par(mfrow=c(1,1))
# I created a simple function to find best m:
functune = function(m){
  sapply(1:6, function(m) ranger(Surv(time, status) ~ .,
                data = veteran_train,
                mtry = m,
                verbose = TRUE,
                write.forest=TRUE,
                num.trees= 1000,
                importance = 'permutation')$prediction.error)
}
# I commented out this line of code to save running time: 
# findM = apply(replicate(100,functune()), 2, which.min)
# save(findM, file='findM')
load(file='findM')
# m = 3 is the optimal m
table(findM)
```



So we fit random survival tree with m = 3. 

```{r}
# fit random survival tree
r_fit <- ranger(Surv(time, status) ~ .,
                data = veteran_train,
                mtry = 3,
                verbose = TRUE,
                write.forest=TRUE,
                num.trees= 1000,
                importance = 'permutation')
```

### Fitted Survival probability

The model gives the fitted survival probability in a table format, where rows represent individual patient and columns represent event time points, from beginning to the end. Below I'm showing a snapshot of the fitted survival probability table.

```{r}
# distinct survival time in training data
# r_fit$unique.death.times
# fitted survival: survival probability
# rows represent individual patient; columns represent event time points
fit.surv = r_fit$survival
fit.survdf = data.frame(fit.surv)
colnames(fit.survdf) <- paste0('T',as.character(r_fit$unique.death.times))
# write.csv(fit.survdf, file = 'fit.survdf.csv')
# fit.survdf = read.csv('fit.survdf.csv')
fit.survdf[1:5,1:6]
```

Then I sampled 4 individuals to plot fitted survival curve.

```{r}
par(mfrow=c(1,1))
set.seed(12345)
individual = sample(1:dim(veteran_train)[1], 4)
plot(r_fit$unique.death.times, fit.surv[individual[1],], type = 'l', col = 'red',xlab='Time', ylab='Survival Probability', main='Fitted Survival Curve (4 individuals)')
lines(r_fit$unique.death.times,fit.surv[individual[2],], type = 'l', col = 'blue',xlab='Time', ylab='Survival Probability')
lines(r_fit$unique.death.times,fit.surv[individual[3],], type = 'l', col = 'green',xlab='Time', ylab='Survival Probability')
lines(r_fit$unique.death.times,fit.surv[individual[4],], type = 'l', col = 'pink',xlab='Time', ylab='Survival Probability')
abline(v=veteran_train[individual,]$time, lty=3, col=c('red','blue','green','pink'))
legend("topright", lwd = 1, col = c('red','blue','green','pink'),
       legend = c('id14', 'id51', 'id80', 'id90'))
veteran_train[individual,]
```

### Variable Importance

The random survival forest model identified karno, celltype, and trt as the three most important predictors of survival time.

```{r}
# r_fit$variable.importance
# plot(r_fit$variable.importance)
data.frame(r_fit$variable.importance) %>% arrange(desc(r_fit.variable.importance))
```


Similarly, in Cox PH model, the interaction effects between treatment and celltype, karno, diagtime, prior are significant, hence can be considered as important in affecting survival probability. 

```{r}
coxm1
```


### Model performance 

To assess the model performance, I choose to predict proportional of survival after 80 days: if survival probability of an individual is over 50% at time T, then he is predicted to be survived. The preidction accuracy at time point 80 is 63%.


```{r}
preds <- predict(r_fit, veteran_test, type = 'response')$survival
preds = data.frame(preds)
colnames(preds) <- paste0('T',as.character(r_fit$unique.death.times))
predEvent = preds$T80 > 0.5
actualEvent = veteran_test$time >= 80
accuracy = sum(predEvent==actualEvent)/length(actualEvent)
table(predEvent, actualEvent)
accuracy
```


To generalize the performance result, I predicted survival rates for all time points. Prediction accuracy is as below:
Note that the x-axis represents the time index, not the actual time point period.

```{r}
# predict more time point:
# exrtact time integer from column names:
times = as.numeric(substr(colnames(preds),start=2,5))
  
predSurv = function(j){
  # predict survival status:
  predEvent = preds[,j] > 0.5
  actualEvent = veteran_test$time >= times[j]
  # return confusion matrix and accuracy
  return(list(table(predEvent, actualEvent),
              sum(predEvent==actualEvent)/length(actualEvent)))
}
# apply the prediction to all time points:
accuracy_list = sapply(1:dim(preds)[2], function(j) predSurv(j)[[2]])
# plot accuracy
plot(accuracy_list,type='o',pch=16, main='Prediction accuracy by time', xlab='Time Index', ylab='Accuracy')
```


